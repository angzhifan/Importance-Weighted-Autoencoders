{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IWAE_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLOgx/I5VnewcFldUdc464",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angzhifan/Importance-Weighted-Autoencoders/blob/main/IWAE_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo9d_7gfWnBN"
      },
      "source": [
        "This is a PyTorch implementation of the IWAE model and VAE model in the paper *Importance Weighted Autoencoders* by Yuri Burda, Roger Grosse & Ruslan Salakhutdinov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIroQNWvVxg7",
        "outputId": "0b47aab3-6940-459c-e11b-c5d2af84699e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#IWAE & VAE\n",
        "#Angzhi (Andrew) Fan, fana@uchicago.edu\n",
        "#Oct 5, 2020\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# VAE or IWAE with one layer\n",
        "class VAE_1(nn.Module):\n",
        "    \n",
        "    def __init__(self, t):\n",
        "        super(VAE_1, self).__init__()\n",
        "        self.k = t\n",
        "        self.fc1 = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3_mu = nn.Linear(200, 50)\n",
        "        self.fc3_sigma = nn.Linear(200, 50)\n",
        "        self.fc4 = nn.Linear(50, 200)\n",
        "        self.fc5 = nn.Linear(200, 200)\n",
        "        self.fc6 = nn.Linear(200,28*28)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,28*28)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mu = self.fc3_mu(x).view(-1,1,50)\n",
        "        log_sigma = self.fc3_sigma(x).view(-1,1,50)\n",
        "        eps = torch.randn_like(mu.repeat(1,self.k,1))\n",
        "        x = mu.repeat(1,self.k,1) + torch.exp(log_sigma.repeat(1,self.k,1))*eps\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x, mu, log_sigma, eps\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9_PmyZ4qvXB"
      },
      "source": [
        "# VAE or IWAE with two layers\n",
        "class VAE_2(nn.Module):\n",
        "    \n",
        "    def __init__(self, t):\n",
        "        super(VAE_2, self).__init__()\n",
        "        self.k = t\n",
        "        self.fc1 = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3_mu = nn.Linear(200, 100)\n",
        "        self.fc3_sigma = nn.Linear(200, 100)\n",
        "        self.fc4 = nn.Linear(100, 100)\n",
        "        self.fc5 = nn.Linear(100, 100)\n",
        "        self.fc6_mu = nn.Linear(100,50)\n",
        "        self.fc6_sigma = nn.Linear(100,50)\n",
        "        self.fc7 = nn.Linear(50,100)\n",
        "        self.fc8 = nn.Linear(100,100)\n",
        "        self.fc9_mu = nn.Linear(100,100)\n",
        "        self.fc9_sigma = nn.Linear(100,100)\n",
        "        self.fc10 = nn.Linear(100,200)\n",
        "        self.fc11 = nn.Linear(200,200)\n",
        "        self.fc12 = nn.Linear(200,28*28)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,28*28)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mu1 = self.fc3_mu(x)\n",
        "        log_sigma1 = self.fc3_sigma(x)\n",
        "        eps1 = torch.randn_like(mu1.repeat(1,self.k,1))\n",
        "        h1 = mu1.repeat(1,self.k,1) + torch.exp(log_sigma1.repeat(1,self.k,1))*eps1\n",
        "        x = torch.tanh(self.fc4(h1))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        mu2 = self.fc6_mu(x)\n",
        "        log_sigma2 = self.fc6_sigma(x)\n",
        "        eps2 = torch.randn_like(mu2)\n",
        "        h2 = mu2 + torch.exp(log_sigma2)*eps2\n",
        "        x = torch.tanh(self.fc7(h2))\n",
        "        x = torch.tanh(self.fc8(x))\n",
        "        mu3 = self.fc9_mu(x)\n",
        "        log_sigma3 = self.fc9_sigma(x)\n",
        "        x = torch.tanh(self.fc10(h1))\n",
        "        x = torch.tanh(self.fc11(x))\n",
        "        x = self.fc12(x)\n",
        "        return x,mu1,mu2,mu3,log_sigma1,log_sigma2,log_sigma3,eps1,eps2\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TtFB2GaWxIW",
        "outputId": "a617123c-2904-4c98-f98f-a59eb47a33e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQPfZKhNcfHh",
        "outputId": "a2208c94-3a1a-4719-bc90-b583f3fbcbff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def load_data():\n",
        "    train_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_train.amat'\n",
        "    valid_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_valid.amat'\n",
        "    test_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_test.amat'\n",
        "    mnist_train = np.concatenate([np.loadtxt(train_file),np.loadtxt(valid_file)])\n",
        "    mnist_test = np.loadtxt(test_file)\n",
        "    return mnist_train, mnist_test\n",
        "\n",
        "mnist_train, mnist_test = load_data()\n",
        "print(mnist_train.shape)\n",
        "print(mnist_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsAplCYllByq",
        "outputId": "18a49967-a19c-4e4d-8139-efd776232852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_function_1(net):\n",
        "    testloader = torch.utils.data.DataLoader(mnist_test, batch_size=20,shuffle=False)\n",
        "    nll = 0.0\n",
        "    A_u = torch.zeros(50)\n",
        "    for i,data in enumerate(testloader, 0):\n",
        "      with torch.no_grad():\n",
        "        test = data.view(-1,1,28*28).to(device)\n",
        "        output = net(test.float())\n",
        "        \n",
        "        # stochastic layer\n",
        "        eps = torch.randn_like(output[2].repeat(1,5000,1))\n",
        "        h = output[1].repeat(1,5000,1) + torch.exp(output[2].repeat(1,5000,1))*eps\n",
        "\n",
        "        # output of x using the new epsilon\n",
        "        output_x = torch.tanh(net.fc4(h))\n",
        "        output_x = torch.tanh(net.fc5(output_x))\n",
        "        output_x = net.fc6(output_x)\n",
        "        log_prob_condi = torch.sum(output_x*test.repeat(1,5000,1)-torch.log(1+torch.exp(output_x)), 2)\n",
        "\n",
        "        # log weights, unnormalized\n",
        "        log_weights = log_prob_condi-(h*h).sum(2)/2+(eps*eps).sum(2)/2+output[2].repeat(1,5000,1).sum(2)\n",
        "\n",
        "        # estimate log likelihood using L_5000\n",
        "        L_5000 = log_weights.max(1)[0].mean()+torch.log(torch.exp(log_weights\n",
        "                        -log_weights.max(1)[0].view(-1,1)).mean(1)).mean()\n",
        "        nll -= L_5000.item()\n",
        "        A_u += output[1].view(-1,50).var(0).cpu()\n",
        "    return nll/500, sum(A_u.detach().numpy()/500>0.01)\n",
        "\n",
        "print(\"Finished loading test function 1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading test function 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmPdm4I-4Is",
        "outputId": "71a41bf4-2b19-45cf-a6d6-27df55863639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test_function_2(net):\n",
        "    testloader = torch.utils.data.DataLoader(mnist_test, batch_size=20,shuffle=False)\n",
        "    nll = 0.0\n",
        "    A_u_1 = torch.zeros(100)\n",
        "    A_u_2 = torch.zeros(50)\n",
        "    for i,data in enumerate(testloader, 0):\n",
        "      with torch.no_grad():\n",
        "        test = data.view(-1,1,28*28).to(device)\n",
        "        output = net(test.float())\n",
        "        \n",
        "        # stochastic layer sampling\n",
        "        eps1 = torch.randn_like(output[1].repeat(1,5000,1))\n",
        "        # stochastic layer h1\n",
        "        h1 = output[1].repeat(1,5000,1) + torch.exp(output[4].repeat(1,5000,1))*eps1\n",
        "        \n",
        "        x = torch.tanh(net.fc4(h1))\n",
        "        x = torch.tanh(net.fc5(x))\n",
        "\n",
        "        # stochastic layer h2\n",
        "        mu2 = net.fc6_mu(x)\n",
        "        log_sigma2 = net.fc6_sigma(x)\n",
        "        eps2 = torch.randn_like(mu2)\n",
        "        \n",
        "        x = torch.tanh(net.fc10(h1))\n",
        "        x = torch.tanh(net.fc11(x))\n",
        "        x = net.fc12(x)\n",
        "\n",
        "        # log conditional prob\n",
        "        log_prob_condi = torch.sum(x*test.repeat(1,5000,1), 2)-torch.sum(torch.log(1+torch.exp(x)), 2)\n",
        "\n",
        "        # log weights, unnormalized\n",
        "        h2 = (mu2+torch.exp(log_sigma2)*eps2)\n",
        "        x = torch.tanh(net.fc7(h2))\n",
        "        x = torch.tanh(net.fc8(x))\n",
        "        mu3 = net.fc9_mu(x)\n",
        "        log_sigma3 = net.fc9_sigma(x)\n",
        "        h1 = h1-mu3\n",
        "        log_p_h1_h2 = -(h1*h1/torch.exp(2*log_sigma3)).sum(2)/2-log_sigma3.sum(2)\n",
        "        log_q_h1_x = -(eps1*eps1).sum(2)/2-output[4].repeat(1,5000,1).sum(2)\n",
        "        log_q_h2_h1 = -(eps2*eps2).sum(2)/2-log_sigma2.sum(2)\n",
        "        log_weights = log_prob_condi+log_p_h1_h2-(h2*h2).sum(2)/2-log_q_h1_x-log_q_h2_h1\n",
        "\n",
        "\n",
        "        # estimate log likelihood using L_5000\n",
        "        L_5000 = log_weights.max(1)[0].mean()+torch.log(torch.exp(log_weights\n",
        "                        -log_weights.max(1)[0].view(-1,1)).mean(1)).mean()\n",
        "        nll -= L_5000.item()\n",
        "        A_u_1 += output[1].view(-1,100).var(0).cpu()\n",
        "        A_u_2 += mu2[:,0,:].var(0).cpu()\n",
        "    return nll/500, (sum(A_u_1.detach().numpy()/500>0.01),sum(A_u_2.detach().numpy()/500>0.01))\n",
        "\n",
        "print(\"Finished loading test function 2\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading test function 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZeO4I5Teswv"
      },
      "source": [
        "mod = 'iwae'\n",
        "layer = 2\n",
        "batch_size = 20\n",
        "k = 5\n",
        "continued = 0\n",
        "\n",
        "if mod == 'iwae':\n",
        "    #this index_vec will be used in the training of iwae\n",
        "    index_vec = torch.tensor([i*k for i in range(batch_size)]).to(device)\n",
        "elif mod == 'vae':\n",
        "    pass\n",
        "else:\n",
        "    raise Exception(\"Invalid Mode\")\n",
        "\n",
        "if layer ==1:\n",
        "    net = VAE_1(k)\n",
        "elif layer == 2:\n",
        "    net = VAE_2(k)\n",
        "else:\n",
        "    raise Exception(\"Invalid Layer Number\")\n",
        "\n",
        "if continued == 0:\n",
        "  decay_num = 0\n",
        "elif continued in [1, 4, 13, 40, 121, 364, 1093, 3280]:\n",
        "  decay_num = sum(np.array([1, 4, 13, 40, 121, 364, 1093, 3280])<=continued)\n",
        "  net.load_state_dict(torch.load('/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_'+str(continued)+'.pth'))\n",
        "elif continued < 3281:\n",
        "  decay_num = sum(np.array([1, 4, 13, 40, 121, 364, 1093, 3280])<=continued)\n",
        "  net.load_state_dict(torch.load('/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_.pth'))\n",
        "else:\n",
        "  raise Exception(\"Invalid Starting Epoch\")\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'w') as outfile:\n",
        "    outfile.write('output of the code '+'\\n'+'author:Angzhi Fan fana@uchicago.edu'+'\\n')\n",
        "    \n",
        "start = time.time()\n",
        "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, \n",
        "                                         shuffle=True, num_workers=2)\n",
        "learning_rate = 0.001/10**(decay_num/7)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate\n",
        "                             , betas=(0.9, 0.999), eps=1e-04)\n",
        "for epoch in range(continued, 122):\n",
        "    if epoch in [1, 4, 13, 40, 121, 364, 1093, 3280]:\n",
        "        PATH = '/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_'+str(epoch)+'.pth'\n",
        "        torch.save(net.state_dict(), PATH)\n",
        "        learning_rate /= 10**(1/7)\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, \n",
        "                                     betas=(0.9, 0.999), eps=1e-04)\n",
        "        with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "            if layer == 1:\n",
        "                nll, A_u = test_function_1(net)\n",
        "            else:\n",
        "                nll, A_u = test_function_2(net)\n",
        "            print('NLL:', nll, 'active units:', A_u)\n",
        "            outfile.write('test average (NLL):'+str(nll)+'\\n')\n",
        "            outfile.write('test average (active units):'+str(A_u)+'\\n')\n",
        "            outfile.write('learning rate decay'+'\\n')\n",
        "        print(\"learning rate=\", learning_rate)\n",
        "    running_loss = 0.0\n",
        "    for i,data in enumerate(trainloader, 0):\n",
        "        train = data.view(-1,1,28*28).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = net(train.float())\n",
        "        log_prob_condi = torch.sum(output[0]*train.repeat(1,k,1)-torch.log(1+torch.exp(output[0])), 2)\n",
        "\n",
        "        if layer == 1:\n",
        "            # stochastic layer\n",
        "            h = (output[1].repeat(1,k,1) + torch.exp(output[2].repeat(1,k,1))*output[3])\n",
        "            # log weights, unnormalized\n",
        "            log_weights = log_prob_condi-(h*h).sum(2)/2+(output[3]*output[3]).sum(2)/2+output[2].repeat(1,k,1).sum(2)\n",
        "        else:\n",
        "            # stochastic layer h1 minus mu3\n",
        "            h1 = output[1].repeat(1,k,1) + torch.exp(output[4].repeat(1,k,1))*output[7]-output[3]\n",
        "            # stochastic layer h2\n",
        "            h2 = (output[2]+torch.exp(output[5])*output[8])\n",
        "            # log weights, unnormalized\n",
        "            log_p_h1_h2 = -(h1*h1/torch.exp(2*output[6])).sum(2)/2-output[6].sum(2)\n",
        "            log_q_h1_x = -(output[7]*output[7]).sum(2)/2-output[4].repeat(1,k,1).sum(2)\n",
        "            log_q_h2_h1 = -(output[8]*output[8]).sum(2)/2-output[5].sum(2)\n",
        "            log_weights = log_prob_condi+log_p_h1_h2-(h2*h2).sum(2)/2-log_q_h1_x-log_q_h2_h1\n",
        "        \n",
        "        log_weights.to(device)\n",
        "        if mod == 'vae':\n",
        "            loss = -log_weights.mean().to(device)\n",
        "        else:\n",
        "            # sample one index from k sets of hidden values\n",
        "            temp = torch.exp(F.log_softmax(log_weights-log_weights.min(1)[0].view(-1,1),1)).to(device)\n",
        "            temp1 = torch.multinomial(temp,1).flatten().to(device)+index_vec\n",
        "            # estimate loss\n",
        "            loss = -torch.take(log_weights, temp1).mean().to(device)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i%500 == 499:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch+1, i + 1, running_loss/500))\n",
        "            with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "                outfile.write('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch+1, i + 1, running_loss/500)+'\\n')\n",
        "            running_loss = 0.0\n",
        "        \n",
        "PATH = '/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_'+'.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Finished Training')\n",
        "with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "    outfile.write('Finished Training'+'\\n'+'time cost:'+str(time.time()-start)+'\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
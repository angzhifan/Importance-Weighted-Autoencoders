{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IWAE_VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLohqHcBvf2LXPiY8SY2Eh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angzhifan/Importance-Weighted-Autoencoders/blob/main/IWAE_VAE.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo9d_7gfWnBN"
      },
      "source": [
        "This is a PyTorch implementation of the IWAE model and VAE model in the paper *Importance Weighted Autoencoders* by Yuri Burda, Roger Grosse & Ruslan Salakhutdinov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIroQNWvVxg7",
        "outputId": "8dd2cb1c-fc2a-4bf6-eb8b-7522d361fa74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#IWAE & VAE\n",
        "#Angzhi (Andrew) Fan, fana@uchicago.edu\n",
        "#Oct 5, 2020\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# VAE or IWAE with one layer\n",
        "class VAE_1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(VAE_1, self).__init__()\n",
        "        self.k = 5\n",
        "        self.fc1 = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3_mu = nn.Linear(200, 50)\n",
        "        self.fc3_sigma = nn.Linear(200, 50)\n",
        "        self.fc4 = nn.Linear(50, 200)\n",
        "        self.fc5 = nn.Linear(200, 200)\n",
        "        self.fc6 = nn.Linear(200,28*28)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,28*28)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mu = self.fc3_mu(x).view(-1,1,50)\n",
        "        log_sigma = self.fc3_sigma(x).view(-1,1,50)\n",
        "        eps = torch.randn_like(mu.repeat(1,self.k,1))\n",
        "        x = mu.repeat(1,self.k,1) + torch.exp(log_sigma.repeat(1,self.k,1))*eps\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x, mu, log_sigma, eps\n",
        "    \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9_PmyZ4qvXB",
        "outputId": "f481cbbc-dc19-4886-fce2-f1e2d8b47ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# VAE or IWAE with two layers\n",
        "class VAE_2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(VAE_2, self).__init__()\n",
        "        self.k = 5\n",
        "        self.fc1 = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3_mu = nn.Linear(200, 100)\n",
        "        self.fc3_sigma = nn.Linear(200, 100)\n",
        "        self.fc4 = nn.Linear(100, 100)\n",
        "        self.fc5 = nn.Linear(100, 100)\n",
        "        self.fc6_mu = nn.Linear(100,50)\n",
        "        self.fc6_sigma = nn.Linear(100,50)\n",
        "        self.fc7 = nn.Linear(50,100)\n",
        "        self.fc8 = nn.Linear(100,100)\n",
        "        self.fc9_mu = nn.Linear(100,100)\n",
        "        self.fc9_sigma = nn.Linear(100,100)\n",
        "        self.fc10 = nn.Linear(100,200)\n",
        "        self.fc11 = nn.Linear(200,200)\n",
        "        self.fc12 = nn.Linear(200,28*28)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,28*28)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mu1 = self.fc3_mu(x)\n",
        "        log_sigma1 = self.fc3_sigma(x)\n",
        "        eps1 = torch.randn_like(mu1.repeat(1,self.k,1))\n",
        "        h1 = mu1.repeat(1,self.k,1) + torch.exp(log_sigma1.repeat(1,self.k,1))*eps1\n",
        "        x = torch.tanh(self.fc4(h1))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        mu2 = self.fc6_mu(x)\n",
        "        log_sigma2 = self.fc6_sigma(x)\n",
        "        eps2 = torch.randn_like(mu2)\n",
        "        h2 = mu2 + torch.exp(log_sigma2)*eps2\n",
        "        x = torch.tanh(self.fc7(h2))\n",
        "        x = torch.tanh(self.fc8(x))\n",
        "        mu3 = self.fc9_mu(x)\n",
        "        log_sigma3 = self.fc9_sigma(x)\n",
        "        x = torch.tanh(self.fc10(h1))\n",
        "        x = torch.tanh(self.fc11(x))\n",
        "        x = self.fc12(x)\n",
        "        return x,mu1,mu2,mu3,log_sigma1,log_sigma2,log_sigma3,eps1,eps2\n",
        "    \n",
        "temp = VAE_2()\n",
        "output = temp(torch.randn(2,1,28*28))\n",
        "for i in output:\n",
        "    print(i.shape)\n",
        "(output[0].min(),output[0].max())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 5, 784])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([2, 5, 50])\n",
            "torch.Size([2, 5, 100])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([2, 5, 50])\n",
            "torch.Size([2, 5, 100])\n",
            "torch.Size([2, 5, 100])\n",
            "torch.Size([2, 5, 50])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.5780, grad_fn=<MinBackward1>),\n",
              " tensor(0.6304, grad_fn=<MaxBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TtFB2GaWxIW",
        "outputId": "6b1053b2-2ece-4ca4-f8fc-df9288532b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQPfZKhNcfHh",
        "outputId": "77de992d-6fcd-4b3a-e720-415b8a1e84f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def load_data():\n",
        "    train_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_train.amat'\n",
        "    valid_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_valid.amat'\n",
        "    test_file = '/content/drive/My Drive/dataset/BinaryMNIST/binarized_mnist_test.amat'\n",
        "    mnist_train = np.concatenate([np.loadtxt(train_file),np.loadtxt(valid_file)])\n",
        "    mnist_test = np.loadtxt(test_file)\n",
        "    return mnist_train, mnist_test\n",
        "\n",
        "mnist_train, mnist_test = load_data()\n",
        "print(mnist_train.shape)\n",
        "print(mnist_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsAplCYllByq",
        "outputId": "355d0964-2646-4d86-c40f-9f9d5ad603f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_function_1(net):\n",
        "    testloader = torch.utils.data.DataLoader(mnist_test, batch_size=20,shuffle=True)\n",
        "    ll = 0.0\n",
        "    A_u = torch.zeros(50)\n",
        "    for i,data in enumerate(testloader, 0):\n",
        "      with torch.no_grad():\n",
        "        test = data.view(-1,1,28*28).to(device)\n",
        "        output = net(test.float())\n",
        "        \n",
        "        # stochastic layer\n",
        "        eps = torch.randn_like(output[2].repeat(1,5000,1))\n",
        "        h = output[1].repeat(1,5000,1) + torch.exp(output[2].repeat(1,5000,1))*eps\n",
        "\n",
        "        # output of x using the new epsilon\n",
        "        output_x = torch.tanh(net.fc4(h))\n",
        "        output_x = torch.tanh(net.fc5(output_x))\n",
        "        output_x = net.fc6(output_x)\n",
        "        log_prob_condi = torch.sum(output_x*test.repeat(1,5000,1)-torch.log(1+torch.exp(output_x)), 2)\n",
        "\n",
        "        # log weights, unnormalized\n",
        "        log_weights = log_prob_condi-(h*h).sum(2)/2+(eps*eps).sum(2)/2+output[2].repeat(1,5000,1).sum(2)\n",
        "\n",
        "        # estimate log likelihood using L_5000\n",
        "        L_5000 = log_weights.max(1)[0].mean()+torch.log(torch.exp(log_weights\n",
        "                        -log_weights.max(1)[0].view(-1,1)).mean(1)).mean()\n",
        "        ll += L_5000.item()\n",
        "        A_u += output[1].view(-1,50).var(0).cpu()\n",
        "    return ll/500, sum(A_u.detach().numpy()/500>0.01)\n",
        "\n",
        "print(\"Finished loading test function 1\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading test function 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmPdm4I-4Is",
        "outputId": "7f879591-ab9b-43ae-f2ac-bef7807e6aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_function_2(net):\n",
        "    testloader = torch.utils.data.DataLoader(mnist_test, batch_size=20,shuffle=True)\n",
        "    ll = 0.0\n",
        "    A_u_1 = torch.zeros(100)\n",
        "    A_u_2 = torch.zeros(50)\n",
        "    for i,data in enumerate(testloader, 0):\n",
        "      with torch.no_grad():\n",
        "        test = data.view(-1,1,28*28).to(device)\n",
        "        output = net(test.float())\n",
        "        \n",
        "        # stochastic layer sampling\n",
        "        eps1 = torch.randn_like(output[1].repeat(1,5000,1))\n",
        "        # stochastic layer h1\n",
        "        h1 = output[1].repeat(1,5000,1) + torch.exp(output[4].repeat(1,5000,1))*eps1\n",
        "        \n",
        "        x = torch.tanh(net.fc4(h1))\n",
        "        x = torch.tanh(net.fc5(x))\n",
        "\n",
        "        # stochastic layer h2\n",
        "        mu2 = net.fc6_mu(x)\n",
        "        log_sigma2 = net.fc6_sigma(x)\n",
        "        eps2 = torch.randn_like(mu2)\n",
        "        \n",
        "        x = torch.tanh(net.fc10(h1))\n",
        "        x = torch.tanh(net.fc11(x))\n",
        "        x = net.fc12(x)\n",
        "\n",
        "        # log conditional prob\n",
        "        log_prob_condi = torch.sum(x*test.repeat(1,5000,1), 2)-torch.sum(torch.log(1+torch.exp(x)), 2)\n",
        "\n",
        "        # log weights, unnormalized\n",
        "        h2 = (mu2+torch.exp(log_sigma2)*eps2)\n",
        "        x = torch.tanh(net.fc7(h2))\n",
        "        x = torch.tanh(net.fc8(x))\n",
        "        mu3 = net.fc9_mu(x)\n",
        "        log_sigma3 = net.fc9_sigma(x)\n",
        "        h1 = h1-mu3\n",
        "        log_p_h1_h2 = -(h1*h1/torch.exp(2*log_sigma3)).sum(2)/2-log_sigma3.sum(2)\n",
        "        log_q_h1_x = -(eps1*eps1).sum(2)/2-output[4].repeat(1,5000,1).sum(2)\n",
        "        log_q_h2_h1 = -(eps2*eps2).sum(2)/2-log_sigma2.sum(2)\n",
        "        log_weights = log_prob_condi+log_p_h1_h2-(h2*h2).sum(2)/2-log_q_h1_x-log_q_h2_h1\n",
        "\n",
        "\n",
        "        # estimate log likelihood using L_5000\n",
        "        L_5000 = log_weights.max(1)[0].mean()+torch.log(torch.exp(log_weights\n",
        "                        -log_weights.max(1)[0].view(-1,1)).mean(1)).mean()\n",
        "        ll += L_5000.item()\n",
        "        A_u_1 += output[1].view(-1,100).var(0).cpu()\n",
        "        A_u_2 += mu2[:,0,:].var(0).cpu()\n",
        "    return ll/500, (sum(A_u_1.detach().numpy()/500>0.01),sum(A_u_2.detach().numpy()/500>0.01))\n",
        "\n",
        "print(\"Finished loading test function 2\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading test function 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZeO4I5Teswv",
        "outputId": "35445943-cae6-4d7f-fe59-68691cd40675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mod = 'vae'\n",
        "layer = 2\n",
        "batch_size = 20\n",
        "k = 5\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if mod == 'iwae':\n",
        "    #this index_vec will be used in the training of iwae\n",
        "    index_vec = torch.tensor([i*k for i in range(batch_size)]).to(device)\n",
        "elif mod == 'vae':\n",
        "    pass\n",
        "else:\n",
        "    raise Exception(\"Invalid Mode\")\n",
        "\n",
        "if layer ==1:\n",
        "    net = VAE_1()\n",
        "elif layer == 2:\n",
        "    net = VAE_2()\n",
        "else:\n",
        "    raise Exception(\"Invalid Layer Number\")\n",
        "\n",
        "\n",
        "net.k = k\n",
        "net.to(device)\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'w') as outfile:\n",
        "    outfile.write('output of the code '+'\\n'+'author:Angzhi Fan fana@uchicago.edu'+'\\n')\n",
        "    \n",
        "start = time.time()\n",
        "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, \n",
        "                                         shuffle=True, num_workers=2)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate\n",
        "                             , betas=(0.9, 0.999), eps=1e-04)\n",
        "for epoch in range(122):\n",
        "    if epoch in [1, 4, 13, 40, 121, 364, 1093]:\n",
        "        PATH = '/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_'+str(epoch)+'.pth'\n",
        "        torch.save(net.state_dict(), PATH)\n",
        "        learning_rate /= 10**(1/7)\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, \n",
        "                                     betas=(0.9, 0.999), eps=1e-04)\n",
        "        with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "            if layer == 1:\n",
        "                ll, A_u = test_function_1(net)\n",
        "            else:\n",
        "                ll, A_u = test_function_2(net)\n",
        "            print('NLL:', ll, 'active units:', A_u)\n",
        "            outfile.write('test average (NLL):'+str(ll)+'\\n')\n",
        "            outfile.write('test average (active units):'+str(A_u)+'\\n')\n",
        "            outfile.write('learning rate decay'+'\\n')\n",
        "        print(\"learning rate=\", learning_rate)\n",
        "    running_loss = 0.0\n",
        "    for i,data in enumerate(trainloader, 0):\n",
        "        train = data.view(-1,1,28*28).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = net(train.float())\n",
        "        log_prob_condi = torch.sum(output[0]*train.repeat(1,k,1)-torch.log(1+torch.exp(output[0])), 2)\n",
        "\n",
        "        if layer == 1:\n",
        "            # stochastic layer\n",
        "            h = (output[1].repeat(1,k,1) + torch.exp(output[2].repeat(1,k,1))*output[3])\n",
        "            # log weights, unnormalized\n",
        "            log_weights = log_prob_condi-(h*h).sum(2)/2+(output[3]*output[3]).sum(2)/2+output[2].repeat(1,k,1).sum(2)\n",
        "        else:\n",
        "            # stochastic layer h1 minus mu3\n",
        "            h1 = output[1].repeat(1,k,1) + torch.exp(output[4].repeat(1,k,1))*output[7]-output[3]\n",
        "            # stochastic layer h2\n",
        "            h2 = (output[2]+torch.exp(output[5])*output[8])\n",
        "            # log weights, unnormalized\n",
        "            log_p_h1_h2 = -(h1*h1/torch.exp(2*output[6])).sum(2)/2-output[6].sum(2)\n",
        "            log_q_h1_x = -(output[7]*output[7]).sum(2)/2-output[4].repeat(1,k,1).sum(2)\n",
        "            log_q_h2_h1 = -(output[8]*output[8]).sum(2)/2-output[5].sum(2)\n",
        "            log_weights = log_prob_condi+log_p_h1_h2-(h2*h2).sum(2)/2-log_q_h1_x-log_q_h2_h1\n",
        "        \n",
        "        log_weights.to(device)\n",
        "        if mod == 'vae':\n",
        "            loss = -log_weights.mean().to(device)\n",
        "        else:\n",
        "            # sample one index from k sets of hidden values\n",
        "            temp = torch.exp(F.log_softmax(log_weights-log_weights.min(1)[0].view(-1,1),1)).to(device)\n",
        "            temp1 = torch.multinomial(temp,1).flatten().to(device)+index_vec\n",
        "            # estimate loss\n",
        "            loss = -torch.take(log_weights, temp1).mean().to(device)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i%500 == 499:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch+1, i + 1, running_loss/500))\n",
        "            with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "                outfile.write('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch+1, i + 1, running_loss/500)+'\\n')\n",
        "            running_loss = 0.0\n",
        "        \n",
        "PATH = '/content/drive/My Drive/IWAE_VAE/model/'+mod+'_net'+'_layer'+str(layer)+'_k'+str(k)+'_'+'.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Finished Training')\n",
        "with open('/content/drive/My Drive/IWAE_VAE/outfile_'+mod+'_layer'+str(layer)+'_k'+str(k)+'_'+'.txt', 'a') as outfile:\n",
        "    outfile.write('Finished Training'+'\\n'+'time cost:'+str(time.time()-start)+'\\n')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 217.874\n",
            "[1,  1000] loss: 192.781\n",
            "[1,  1500] loss: 186.768\n",
            "[1,  2000] loss: 174.609\n",
            "[1,  2500] loss: 168.341\n",
            "[1,  3000] loss: 160.428\n",
            "NLL: -150.2283498720555 active units: (63, 0)\n",
            "learning rate= 0.0007196856730011521\n",
            "[2,   500] loss: 151.735\n",
            "[2,  1000] loss: 147.431\n",
            "[2,  1500] loss: 144.088\n",
            "[2,  2000] loss: 141.115\n",
            "[2,  2500] loss: 138.400\n",
            "[2,  3000] loss: 136.627\n",
            "[3,   500] loss: 134.368\n",
            "[3,  1000] loss: 132.609\n",
            "[3,  1500] loss: 132.354\n",
            "[3,  2000] loss: 129.403\n",
            "[3,  2500] loss: 128.327\n",
            "[3,  3000] loss: 127.818\n",
            "[4,   500] loss: 126.930\n",
            "[4,  1000] loss: 126.093\n",
            "[4,  1500] loss: 125.292\n",
            "[4,  2000] loss: 124.733\n",
            "[4,  2500] loss: 124.176\n",
            "[4,  3000] loss: 123.331\n",
            "NLL: -117.33824765750248 active units: (39, 0)\n",
            "learning rate= 0.0005179474679231211\n",
            "[5,   500] loss: 121.906\n",
            "[5,  1000] loss: 120.829\n",
            "[5,  1500] loss: 121.098\n",
            "[5,  2000] loss: 120.330\n",
            "[5,  2500] loss: 120.043\n",
            "[5,  3000] loss: 119.892\n",
            "[6,   500] loss: 118.530\n",
            "[6,  1000] loss: 117.721\n",
            "[6,  1500] loss: 118.324\n",
            "[6,  2000] loss: 117.933\n",
            "[6,  2500] loss: 117.648\n",
            "[6,  3000] loss: 117.181\n",
            "[7,   500] loss: 117.187\n",
            "[7,  1000] loss: 115.674\n",
            "[7,  1500] loss: 114.810\n",
            "[7,  2000] loss: 115.840\n",
            "[7,  2500] loss: 115.240\n",
            "[7,  3000] loss: 115.625\n",
            "[8,   500] loss: 114.463\n",
            "[8,  1000] loss: 114.551\n",
            "[8,  1500] loss: 114.731\n",
            "[8,  2000] loss: 114.073\n",
            "[8,  2500] loss: 113.893\n",
            "[8,  3000] loss: 113.317\n",
            "[9,   500] loss: 113.252\n",
            "[9,  1000] loss: 113.710\n",
            "[9,  1500] loss: 112.762\n",
            "[9,  2000] loss: 112.797\n",
            "[9,  2500] loss: 112.659\n",
            "[9,  3000] loss: 112.524\n",
            "[10,   500] loss: 112.451\n",
            "[10,  1000] loss: 112.027\n",
            "[10,  1500] loss: 111.274\n",
            "[10,  2000] loss: 111.360\n",
            "[10,  2500] loss: 111.877\n",
            "[10,  3000] loss: 112.280\n",
            "[11,   500] loss: 111.111\n",
            "[11,  1000] loss: 111.165\n",
            "[11,  1500] loss: 110.647\n",
            "[11,  2000] loss: 111.238\n",
            "[11,  2500] loss: 110.859\n",
            "[11,  3000] loss: 110.535\n",
            "[12,   500] loss: 110.378\n",
            "[12,  1000] loss: 110.372\n",
            "[12,  1500] loss: 109.475\n",
            "[12,  2000] loss: 109.949\n",
            "[12,  2500] loss: 109.575\n",
            "[12,  3000] loss: 110.041\n",
            "[13,   500] loss: 109.724\n",
            "[13,  1000] loss: 109.492\n",
            "[13,  1500] loss: 109.408\n",
            "[13,  2000] loss: 108.779\n",
            "[13,  2500] loss: 109.068\n",
            "[13,  3000] loss: 109.085\n",
            "NLL: -104.3015760934853 active units: (9, 0)\n",
            "learning rate= 0.000372759372031494\n",
            "[14,   500] loss: 107.876\n",
            "[14,  1000] loss: 108.252\n",
            "[14,  1500] loss: 107.950\n",
            "[14,  2000] loss: 107.711\n",
            "[14,  2500] loss: 108.009\n",
            "[14,  3000] loss: 107.659\n",
            "[15,   500] loss: 106.904\n",
            "[15,  1000] loss: 107.015\n",
            "[15,  1500] loss: 107.348\n",
            "[15,  2000] loss: 107.710\n",
            "[15,  2500] loss: 107.752\n",
            "[15,  3000] loss: 107.813\n",
            "[16,   500] loss: 106.466\n",
            "[16,  1000] loss: 107.325\n",
            "[16,  1500] loss: 107.314\n",
            "[16,  2000] loss: 107.136\n",
            "[16,  2500] loss: 107.030\n",
            "[16,  3000] loss: 106.921\n",
            "[17,   500] loss: 106.108\n",
            "[17,  1000] loss: 106.409\n",
            "[17,  1500] loss: 106.595\n",
            "[17,  2000] loss: 106.702\n",
            "[17,  2500] loss: 106.709\n",
            "[17,  3000] loss: 107.608\n",
            "[18,   500] loss: 106.206\n",
            "[18,  1000] loss: 105.921\n",
            "[18,  1500] loss: 106.513\n",
            "[18,  2000] loss: 106.737\n",
            "[18,  2500] loss: 106.461\n",
            "[18,  3000] loss: 106.315\n",
            "[19,   500] loss: 105.846\n",
            "[19,  1000] loss: 105.775\n",
            "[19,  1500] loss: 105.978\n",
            "[19,  2000] loss: 106.088\n",
            "[19,  2500] loss: 106.024\n",
            "[19,  3000] loss: 106.629\n",
            "[20,   500] loss: 105.305\n",
            "[20,  1000] loss: 105.131\n",
            "[20,  1500] loss: 106.147\n",
            "[20,  2000] loss: 106.323\n",
            "[20,  2500] loss: 105.853\n",
            "[20,  3000] loss: 105.783\n",
            "[21,   500] loss: 105.553\n",
            "[21,  1000] loss: 105.490\n",
            "[21,  1500] loss: 105.454\n",
            "[21,  2000] loss: 105.541\n",
            "[21,  2500] loss: 105.075\n",
            "[21,  3000] loss: 105.807\n",
            "[22,   500] loss: 105.024\n",
            "[22,  1000] loss: 105.463\n",
            "[22,  1500] loss: 104.738\n",
            "[22,  2000] loss: 105.106\n",
            "[22,  2500] loss: 105.144\n",
            "[22,  3000] loss: 105.765\n",
            "[23,   500] loss: 104.920\n",
            "[23,  1000] loss: 104.501\n",
            "[23,  1500] loss: 105.042\n",
            "[23,  2000] loss: 105.271\n",
            "[23,  2500] loss: 105.117\n",
            "[23,  3000] loss: 104.908\n",
            "[24,   500] loss: 104.620\n",
            "[24,  1000] loss: 104.879\n",
            "[24,  1500] loss: 104.244\n",
            "[24,  2000] loss: 104.674\n",
            "[24,  2500] loss: 105.263\n",
            "[24,  3000] loss: 104.647\n",
            "[25,   500] loss: 103.982\n",
            "[25,  1000] loss: 104.512\n",
            "[25,  1500] loss: 104.238\n",
            "[25,  2000] loss: 104.657\n",
            "[25,  2500] loss: 104.847\n",
            "[25,  3000] loss: 104.686\n",
            "[26,   500] loss: 104.374\n",
            "[26,  1000] loss: 103.954\n",
            "[26,  1500] loss: 104.429\n",
            "[26,  2000] loss: 104.283\n",
            "[26,  2500] loss: 103.965\n",
            "[26,  3000] loss: 104.294\n",
            "[27,   500] loss: 102.736\n",
            "[27,  1000] loss: 104.521\n",
            "[27,  1500] loss: 104.180\n",
            "[27,  2000] loss: 104.070\n",
            "[27,  2500] loss: 103.856\n",
            "[27,  3000] loss: 103.878\n",
            "[28,   500] loss: 102.858\n",
            "[28,  1000] loss: 103.945\n",
            "[28,  1500] loss: 103.773\n",
            "[28,  2000] loss: 104.275\n",
            "[28,  2500] loss: 103.155\n",
            "[28,  3000] loss: 103.815\n",
            "[29,   500] loss: 103.357\n",
            "[29,  1000] loss: 102.914\n",
            "[29,  1500] loss: 103.329\n",
            "[29,  2000] loss: 103.555\n",
            "[29,  2500] loss: 103.627\n",
            "[29,  3000] loss: 103.759\n",
            "[30,   500] loss: 102.921\n",
            "[30,  1000] loss: 103.261\n",
            "[30,  1500] loss: 103.624\n",
            "[30,  2000] loss: 103.016\n",
            "[30,  2500] loss: 103.058\n",
            "[30,  3000] loss: 103.355\n",
            "[31,   500] loss: 102.897\n",
            "[31,  1000] loss: 103.117\n",
            "[31,  1500] loss: 102.768\n",
            "[31,  2000] loss: 103.273\n",
            "[31,  2500] loss: 103.181\n",
            "[31,  3000] loss: 102.839\n",
            "[32,   500] loss: 102.804\n",
            "[32,  1000] loss: 102.834\n",
            "[32,  1500] loss: 103.167\n",
            "[32,  2000] loss: 102.744\n",
            "[32,  2500] loss: 102.704\n",
            "[32,  3000] loss: 102.779\n",
            "[33,   500] loss: 102.273\n",
            "[33,  1000] loss: 102.909\n",
            "[33,  1500] loss: 102.648\n",
            "[33,  2000] loss: 102.383\n",
            "[33,  2500] loss: 102.703\n",
            "[33,  3000] loss: 103.110\n",
            "[34,   500] loss: 102.260\n",
            "[34,  1000] loss: 102.228\n",
            "[34,  1500] loss: 102.408\n",
            "[34,  2000] loss: 102.856\n",
            "[34,  2500] loss: 102.626\n",
            "[34,  3000] loss: 102.605\n",
            "[35,   500] loss: 102.128\n",
            "[35,  1000] loss: 101.747\n",
            "[35,  1500] loss: 102.798\n",
            "[35,  2000] loss: 102.428\n",
            "[35,  2500] loss: 102.253\n",
            "[35,  3000] loss: 102.826\n",
            "[36,   500] loss: 101.529\n",
            "[36,  1000] loss: 102.093\n",
            "[36,  1500] loss: 102.622\n",
            "[36,  2000] loss: 102.063\n",
            "[36,  2500] loss: 102.674\n",
            "[36,  3000] loss: 102.333\n",
            "[37,   500] loss: 101.727\n",
            "[37,  1000] loss: 101.966\n",
            "[37,  1500] loss: 102.376\n",
            "[37,  2000] loss: 102.384\n",
            "[37,  2500] loss: 101.775\n",
            "[37,  3000] loss: 102.146\n",
            "[38,   500] loss: 101.783\n",
            "[38,  1000] loss: 102.171\n",
            "[38,  1500] loss: 101.517\n",
            "[38,  2000] loss: 101.752\n",
            "[38,  2500] loss: 102.198\n",
            "[38,  3000] loss: 102.206\n",
            "[39,   500] loss: 101.739\n",
            "[39,  1000] loss: 101.769\n",
            "[39,  1500] loss: 101.913\n",
            "[39,  2000] loss: 101.818\n",
            "[39,  2500] loss: 101.655\n",
            "[39,  3000] loss: 101.936\n",
            "[40,   500] loss: 101.602\n",
            "[40,  1000] loss: 101.521\n",
            "[40,  1500] loss: 101.282\n",
            "[40,  2000] loss: 101.647\n",
            "[40,  2500] loss: 101.944\n",
            "[40,  3000] loss: 102.062\n",
            "NLL: -98.12091818802263 active units: (10, 0)\n",
            "learning rate= 0.00026826957952797256\n",
            "[41,   500] loss: 100.009\n",
            "[41,  1000] loss: 101.060\n",
            "[41,  1500] loss: 101.086\n",
            "[41,  2000] loss: 101.365\n",
            "[41,  2500] loss: 101.289\n",
            "[41,  3000] loss: 100.719\n",
            "[42,   500] loss: 100.930\n",
            "[42,  1000] loss: 100.319\n",
            "[42,  1500] loss: 100.792\n",
            "[42,  2000] loss: 100.500\n",
            "[42,  2500] loss: 101.362\n",
            "[42,  3000] loss: 100.755\n",
            "[43,   500] loss: 100.687\n",
            "[43,  1000] loss: 100.925\n",
            "[43,  1500] loss: 100.556\n",
            "[43,  2000] loss: 99.958\n",
            "[43,  2500] loss: 101.117\n",
            "[43,  3000] loss: 100.735\n",
            "[44,   500] loss: 100.228\n",
            "[44,  1000] loss: 100.446\n",
            "[44,  1500] loss: 100.239\n",
            "[44,  2000] loss: 100.754\n",
            "[44,  2500] loss: 100.515\n",
            "[44,  3000] loss: 101.323\n",
            "[45,   500] loss: 100.218\n",
            "[45,  1000] loss: 100.467\n",
            "[45,  1500] loss: 100.672\n",
            "[45,  2000] loss: 100.749\n",
            "[45,  2500] loss: 100.208\n",
            "[45,  3000] loss: 100.620\n",
            "[46,   500] loss: 100.555\n",
            "[46,  1000] loss: 100.320\n",
            "[46,  1500] loss: 100.607\n",
            "[46,  2000] loss: 100.922\n",
            "[46,  2500] loss: 100.047\n",
            "[46,  3000] loss: 100.014\n",
            "[47,   500] loss: 100.331\n",
            "[47,  1000] loss: 100.295\n",
            "[47,  1500] loss: 100.151\n",
            "[47,  2000] loss: 100.717\n",
            "[47,  2500] loss: 100.148\n",
            "[47,  3000] loss: 100.359\n",
            "[48,   500] loss: 100.256\n",
            "[48,  1000] loss: 100.179\n",
            "[48,  1500] loss: 100.731\n",
            "[48,  2000] loss: 99.650\n",
            "[48,  2500] loss: 100.227\n",
            "[48,  3000] loss: 100.452\n",
            "[49,   500] loss: 100.398\n",
            "[49,  1000] loss: 100.067\n",
            "[49,  1500] loss: 100.344\n",
            "[49,  2000] loss: 100.072\n",
            "[49,  2500] loss: 100.081\n",
            "[49,  3000] loss: 100.120\n",
            "[50,   500] loss: 99.739\n",
            "[50,  1000] loss: 99.785\n",
            "[50,  1500] loss: 100.838\n",
            "[50,  2000] loss: 100.327\n",
            "[50,  2500] loss: 99.932\n",
            "[50,  3000] loss: 100.087\n",
            "[51,   500] loss: 100.226\n",
            "[51,  1000] loss: 99.993\n",
            "[51,  1500] loss: 99.812\n",
            "[51,  2000] loss: 100.139\n",
            "[51,  2500] loss: 100.368\n",
            "[51,  3000] loss: 99.665\n",
            "[52,   500] loss: 100.093\n",
            "[52,  1000] loss: 99.915\n",
            "[52,  1500] loss: 100.146\n",
            "[52,  2000] loss: 99.742\n",
            "[52,  2500] loss: 99.638\n",
            "[52,  3000] loss: 100.299\n",
            "[53,   500] loss: 99.927\n",
            "[53,  1000] loss: 99.469\n",
            "[53,  1500] loss: 100.257\n",
            "[53,  2000] loss: 99.581\n",
            "[53,  2500] loss: 100.307\n",
            "[53,  3000] loss: 99.851\n",
            "[54,   500] loss: 99.479\n",
            "[54,  1000] loss: 99.829\n",
            "[54,  1500] loss: 99.864\n",
            "[54,  2000] loss: 99.729\n",
            "[54,  2500] loss: 100.177\n",
            "[54,  3000] loss: 99.920\n",
            "[55,   500] loss: 99.336\n",
            "[55,  1000] loss: 99.759\n",
            "[55,  1500] loss: 99.797\n",
            "[55,  2000] loss: 99.846\n",
            "[55,  2500] loss: 99.922\n",
            "[55,  3000] loss: 99.983\n",
            "[56,   500] loss: 99.485\n",
            "[56,  1000] loss: 99.284\n",
            "[56,  1500] loss: 99.490\n",
            "[56,  2000] loss: 99.746\n",
            "[56,  2500] loss: 100.196\n",
            "[56,  3000] loss: 99.995\n",
            "[57,   500] loss: 99.185\n",
            "[57,  1000] loss: 100.007\n",
            "[57,  1500] loss: 99.794\n",
            "[57,  2000] loss: 99.608\n",
            "[57,  2500] loss: 99.872\n",
            "[57,  3000] loss: 99.331\n",
            "[58,   500] loss: 99.074\n",
            "[58,  1000] loss: 99.668\n",
            "[58,  1500] loss: 99.156\n",
            "[58,  2000] loss: 99.858\n",
            "[58,  2500] loss: 100.159\n",
            "[58,  3000] loss: 99.765\n",
            "[59,   500] loss: 99.482\n",
            "[59,  1000] loss: 99.184\n",
            "[59,  1500] loss: 99.488\n",
            "[59,  2000] loss: 99.683\n",
            "[59,  2500] loss: 99.872\n",
            "[59,  3000] loss: 99.473\n",
            "[60,   500] loss: 99.117\n",
            "[60,  1000] loss: 99.216\n",
            "[60,  1500] loss: 100.130\n",
            "[60,  2000] loss: 99.215\n",
            "[60,  2500] loss: 99.600\n",
            "[60,  3000] loss: 99.521\n",
            "[61,   500] loss: 99.371\n",
            "[61,  1000] loss: 99.303\n",
            "[61,  1500] loss: 99.798\n",
            "[61,  2000] loss: 99.572\n",
            "[61,  2500] loss: 99.681\n",
            "[61,  3000] loss: 98.713\n",
            "[62,   500] loss: 99.716\n",
            "[62,  1000] loss: 99.202\n",
            "[62,  1500] loss: 98.929\n",
            "[62,  2000] loss: 99.160\n",
            "[62,  2500] loss: 99.615\n",
            "[62,  3000] loss: 99.587\n",
            "[63,   500] loss: 99.122\n",
            "[63,  1000] loss: 99.194\n",
            "[63,  1500] loss: 99.341\n",
            "[63,  2000] loss: 99.405\n",
            "[63,  2500] loss: 99.621\n",
            "[63,  3000] loss: 99.117\n",
            "[64,   500] loss: 98.973\n",
            "[64,  1000] loss: 99.246\n",
            "[64,  1500] loss: 98.976\n",
            "[64,  2000] loss: 99.139\n",
            "[64,  2500] loss: 99.350\n",
            "[64,  3000] loss: 99.848\n",
            "[65,   500] loss: 98.662\n",
            "[65,  1000] loss: 99.247\n",
            "[65,  1500] loss: 99.488\n",
            "[65,  2000] loss: 99.133\n",
            "[65,  2500] loss: 99.523\n",
            "[65,  3000] loss: 99.035\n",
            "[66,   500] loss: 99.633\n",
            "[66,  1000] loss: 99.036\n",
            "[66,  1500] loss: 99.272\n",
            "[66,  2000] loss: 99.032\n",
            "[66,  2500] loss: 98.731\n",
            "[66,  3000] loss: 99.130\n",
            "[67,   500] loss: 98.797\n",
            "[67,  1000] loss: 98.925\n",
            "[67,  1500] loss: 99.176\n",
            "[67,  2000] loss: 99.004\n",
            "[67,  2500] loss: 99.152\n",
            "[67,  3000] loss: 99.655\n",
            "[68,   500] loss: 98.809\n",
            "[68,  1000] loss: 99.161\n",
            "[68,  1500] loss: 99.092\n",
            "[68,  2000] loss: 99.408\n",
            "[68,  2500] loss: 98.532\n",
            "[68,  3000] loss: 99.330\n",
            "[69,   500] loss: 98.775\n",
            "[69,  1000] loss: 98.598\n",
            "[69,  1500] loss: 99.655\n",
            "[69,  2000] loss: 98.638\n",
            "[69,  2500] loss: 99.237\n",
            "[69,  3000] loss: 99.125\n",
            "[70,   500] loss: 98.935\n",
            "[70,  1000] loss: 98.794\n",
            "[70,  1500] loss: 98.972\n",
            "[70,  2000] loss: 99.293\n",
            "[70,  2500] loss: 98.940\n",
            "[70,  3000] loss: 98.809\n",
            "[71,   500] loss: 98.583\n",
            "[71,  1000] loss: 98.463\n",
            "[71,  1500] loss: 99.477\n",
            "[71,  2000] loss: 98.673\n",
            "[71,  2500] loss: 99.168\n",
            "[71,  3000] loss: 99.177\n",
            "[72,   500] loss: 99.013\n",
            "[72,  1000] loss: 98.530\n",
            "[72,  1500] loss: 98.872\n",
            "[72,  2000] loss: 98.904\n",
            "[72,  2500] loss: 98.605\n",
            "[72,  3000] loss: 99.199\n",
            "[73,   500] loss: 99.135\n",
            "[73,  1000] loss: 98.611\n",
            "[73,  1500] loss: 98.563\n",
            "[73,  2000] loss: 98.726\n",
            "[73,  2500] loss: 99.131\n",
            "[73,  3000] loss: 98.746\n",
            "[74,   500] loss: 98.837\n",
            "[74,  1000] loss: 98.647\n",
            "[74,  1500] loss: 98.424\n",
            "[74,  2000] loss: 98.854\n",
            "[74,  2500] loss: 99.286\n",
            "[74,  3000] loss: 98.699\n",
            "[75,   500] loss: 98.429\n",
            "[75,  1000] loss: 98.895\n",
            "[75,  1500] loss: 98.992\n",
            "[75,  2000] loss: 98.823\n",
            "[75,  2500] loss: 98.742\n",
            "[75,  3000] loss: 98.644\n",
            "[76,   500] loss: 98.371\n",
            "[76,  1000] loss: 98.650\n",
            "[76,  1500] loss: 98.574\n",
            "[76,  2000] loss: 98.953\n",
            "[76,  2500] loss: 98.830\n",
            "[76,  3000] loss: 98.823\n",
            "[77,   500] loss: 98.163\n",
            "[77,  1000] loss: 98.794\n",
            "[77,  1500] loss: 98.423\n",
            "[77,  2000] loss: 98.990\n",
            "[77,  2500] loss: 99.037\n",
            "[77,  3000] loss: 98.539\n",
            "[78,   500] loss: 98.150\n",
            "[78,  1000] loss: 98.657\n",
            "[78,  1500] loss: 98.512\n",
            "[78,  2000] loss: 98.495\n",
            "[78,  2500] loss: 99.352\n",
            "[78,  3000] loss: 98.625\n",
            "[79,   500] loss: 98.233\n",
            "[79,  1000] loss: 98.645\n",
            "[79,  1500] loss: 98.750\n",
            "[79,  2000] loss: 98.019\n",
            "[79,  2500] loss: 98.749\n",
            "[79,  3000] loss: 99.118\n",
            "[80,   500] loss: 97.766\n",
            "[80,  1000] loss: 98.435\n",
            "[80,  1500] loss: 98.722\n",
            "[80,  2000] loss: 99.060\n",
            "[80,  2500] loss: 98.345\n",
            "[80,  3000] loss: 98.886\n",
            "[81,   500] loss: 98.854\n",
            "[81,  1000] loss: 98.261\n",
            "[81,  1500] loss: 98.486\n",
            "[81,  2000] loss: 97.968\n",
            "[81,  2500] loss: 98.793\n",
            "[81,  3000] loss: 98.680\n",
            "[82,   500] loss: 98.539\n",
            "[82,  1000] loss: 98.532\n",
            "[82,  1500] loss: 98.629\n",
            "[82,  2000] loss: 98.416\n",
            "[82,  2500] loss: 98.274\n",
            "[82,  3000] loss: 98.476\n",
            "[83,   500] loss: 98.749\n",
            "[83,  1000] loss: 98.264\n",
            "[83,  1500] loss: 98.336\n",
            "[83,  2000] loss: 98.666\n",
            "[83,  2500] loss: 98.215\n",
            "[83,  3000] loss: 98.478\n",
            "[84,   500] loss: 98.126\n",
            "[84,  1000] loss: 98.614\n",
            "[84,  1500] loss: 97.904\n",
            "[84,  2000] loss: 98.385\n",
            "[84,  2500] loss: 98.836\n",
            "[84,  3000] loss: 98.552\n",
            "[85,   500] loss: 98.206\n",
            "[85,  1000] loss: 98.129\n",
            "[85,  1500] loss: 98.495\n",
            "[85,  2000] loss: 98.484\n",
            "[85,  2500] loss: 98.660\n",
            "[85,  3000] loss: 98.251\n",
            "[86,   500] loss: 97.955\n",
            "[86,  1000] loss: 98.516\n",
            "[86,  1500] loss: 98.560\n",
            "[86,  2000] loss: 98.342\n",
            "[86,  2500] loss: 98.417\n",
            "[86,  3000] loss: 98.244\n",
            "[87,   500] loss: 98.521\n",
            "[87,  1000] loss: 98.119\n",
            "[87,  1500] loss: 98.308\n",
            "[87,  2000] loss: 98.400\n",
            "[87,  2500] loss: 98.316\n",
            "[87,  3000] loss: 98.115\n",
            "[88,   500] loss: 98.090\n",
            "[88,  1000] loss: 98.165\n",
            "[88,  1500] loss: 98.467\n",
            "[88,  2000] loss: 98.285\n",
            "[88,  2500] loss: 98.275\n",
            "[88,  3000] loss: 98.378\n",
            "[89,   500] loss: 98.413\n",
            "[89,  1000] loss: 98.174\n",
            "[89,  1500] loss: 98.067\n",
            "[89,  2000] loss: 98.387\n",
            "[89,  2500] loss: 98.339\n",
            "[89,  3000] loss: 98.022\n",
            "[90,   500] loss: 97.663\n",
            "[90,  1000] loss: 98.488\n",
            "[90,  1500] loss: 98.103\n",
            "[90,  2000] loss: 98.281\n",
            "[90,  2500] loss: 98.561\n",
            "[90,  3000] loss: 98.146\n",
            "[91,   500] loss: 98.618\n",
            "[91,  1000] loss: 97.967\n",
            "[91,  1500] loss: 98.193\n",
            "[91,  2000] loss: 98.696\n",
            "[91,  2500] loss: 97.837\n",
            "[91,  3000] loss: 97.784\n",
            "[92,   500] loss: 98.275\n",
            "[92,  1000] loss: 98.046\n",
            "[92,  1500] loss: 97.448\n",
            "[92,  2000] loss: 98.104\n",
            "[92,  2500] loss: 98.451\n",
            "[92,  3000] loss: 98.474\n",
            "[93,   500] loss: 97.718\n",
            "[93,  1000] loss: 98.231\n",
            "[93,  1500] loss: 98.253\n",
            "[93,  2000] loss: 98.055\n",
            "[93,  2500] loss: 98.382\n",
            "[93,  3000] loss: 98.046\n",
            "[94,   500] loss: 97.538\n",
            "[94,  1000] loss: 97.755\n",
            "[94,  1500] loss: 98.050\n",
            "[94,  2000] loss: 98.474\n",
            "[94,  2500] loss: 98.577\n",
            "[94,  3000] loss: 98.113\n",
            "[95,   500] loss: 97.509\n",
            "[95,  1000] loss: 97.653\n",
            "[95,  1500] loss: 97.626\n",
            "[95,  2000] loss: 98.491\n",
            "[95,  2500] loss: 98.576\n",
            "[95,  3000] loss: 98.444\n",
            "[96,   500] loss: 97.640\n",
            "[96,  1000] loss: 97.984\n",
            "[96,  1500] loss: 97.996\n",
            "[96,  2000] loss: 97.890\n",
            "[96,  2500] loss: 98.294\n",
            "[96,  3000] loss: 98.336\n",
            "[97,   500] loss: 97.787\n",
            "[97,  1000] loss: 98.070\n",
            "[97,  1500] loss: 97.766\n",
            "[97,  2000] loss: 97.948\n",
            "[97,  2500] loss: 98.268\n",
            "[97,  3000] loss: 98.128\n",
            "[98,   500] loss: 97.899\n",
            "[98,  1000] loss: 97.772\n",
            "[98,  1500] loss: 97.966\n",
            "[98,  2000] loss: 98.187\n",
            "[98,  2500] loss: 97.889\n",
            "[98,  3000] loss: 98.066\n",
            "[99,   500] loss: 97.228\n",
            "[99,  1000] loss: 97.818\n",
            "[99,  1500] loss: 98.181\n",
            "[99,  2000] loss: 97.792\n",
            "[99,  2500] loss: 98.499\n",
            "[99,  3000] loss: 98.091\n",
            "[100,   500] loss: 98.307\n",
            "[100,  1000] loss: 98.021\n",
            "[100,  1500] loss: 97.813\n",
            "[100,  2000] loss: 97.521\n",
            "[100,  2500] loss: 97.907\n",
            "[100,  3000] loss: 97.794\n",
            "[101,   500] loss: 97.752\n",
            "[101,  1000] loss: 97.638\n",
            "[101,  1500] loss: 97.701\n",
            "[101,  2000] loss: 98.042\n",
            "[101,  2500] loss: 98.085\n",
            "[101,  3000] loss: 98.050\n",
            "[102,   500] loss: 97.680\n",
            "[102,  1000] loss: 97.586\n",
            "[102,  1500] loss: 97.931\n",
            "[102,  2000] loss: 98.032\n",
            "[102,  2500] loss: 98.095\n",
            "[102,  3000] loss: 97.831\n",
            "[103,   500] loss: 97.935\n",
            "[103,  1000] loss: 97.467\n",
            "[103,  1500] loss: 97.802\n",
            "[103,  2000] loss: 98.175\n",
            "[103,  2500] loss: 97.699\n",
            "[103,  3000] loss: 97.855\n",
            "[104,   500] loss: 98.227\n",
            "[104,  1000] loss: 97.438\n",
            "[104,  1500] loss: 97.870\n",
            "[104,  2000] loss: 97.332\n",
            "[104,  2500] loss: 97.837\n",
            "[104,  3000] loss: 98.037\n",
            "[105,   500] loss: 97.113\n",
            "[105,  1000] loss: 97.592\n",
            "[105,  1500] loss: 98.174\n",
            "[105,  2000] loss: 97.958\n",
            "[105,  2500] loss: 97.732\n",
            "[105,  3000] loss: 98.124\n",
            "[106,   500] loss: 97.221\n",
            "[106,  1000] loss: 98.243\n",
            "[106,  1500] loss: 97.772\n",
            "[106,  2000] loss: 97.561\n",
            "[106,  2500] loss: 97.908\n",
            "[106,  3000] loss: 97.789\n",
            "[107,   500] loss: 97.608\n",
            "[107,  1000] loss: 97.899\n",
            "[107,  1500] loss: 97.314\n",
            "[107,  2000] loss: 97.708\n",
            "[107,  2500] loss: 97.744\n",
            "[107,  3000] loss: 98.066\n",
            "[108,   500] loss: 97.389\n",
            "[108,  1000] loss: 97.871\n",
            "[108,  1500] loss: 97.835\n",
            "[108,  2000] loss: 97.713\n",
            "[108,  2500] loss: 97.611\n",
            "[108,  3000] loss: 97.789\n",
            "[109,   500] loss: 97.452\n",
            "[109,  1000] loss: 98.051\n",
            "[109,  1500] loss: 97.589\n",
            "[109,  2000] loss: 97.399\n",
            "[109,  2500] loss: 97.744\n",
            "[109,  3000] loss: 97.810\n",
            "[110,   500] loss: 97.422\n",
            "[110,  1000] loss: 97.607\n",
            "[110,  1500] loss: 98.003\n",
            "[110,  2000] loss: 97.126\n",
            "[110,  2500] loss: 97.969\n",
            "[110,  3000] loss: 97.729\n",
            "[111,   500] loss: 97.653\n",
            "[111,  1000] loss: 97.621\n",
            "[111,  1500] loss: 97.143\n",
            "[111,  2000] loss: 97.732\n",
            "[111,  2500] loss: 97.596\n",
            "[111,  3000] loss: 97.936\n",
            "[112,   500] loss: 97.493\n",
            "[112,  1000] loss: 97.196\n",
            "[112,  1500] loss: 97.264\n",
            "[112,  2000] loss: 97.983\n",
            "[112,  2500] loss: 97.849\n",
            "[112,  3000] loss: 97.802\n",
            "[113,   500] loss: 97.858\n",
            "[113,  1000] loss: 97.692\n",
            "[113,  1500] loss: 97.251\n",
            "[113,  2000] loss: 97.506\n",
            "[113,  2500] loss: 97.673\n",
            "[113,  3000] loss: 97.501\n",
            "[114,   500] loss: 97.006\n",
            "[114,  1000] loss: 97.780\n",
            "[114,  1500] loss: 97.518\n",
            "[114,  2000] loss: 97.764\n",
            "[114,  2500] loss: 97.346\n",
            "[114,  3000] loss: 97.836\n",
            "[115,   500] loss: 97.078\n",
            "[115,  1000] loss: 98.021\n",
            "[115,  1500] loss: 97.512\n",
            "[115,  2000] loss: 97.592\n",
            "[115,  2500] loss: 97.430\n",
            "[115,  3000] loss: 97.488\n",
            "[116,   500] loss: 97.454\n",
            "[116,  1000] loss: 97.346\n",
            "[116,  1500] loss: 97.862\n",
            "[116,  2000] loss: 97.496\n",
            "[116,  2500] loss: 97.497\n",
            "[116,  3000] loss: 97.329\n",
            "[117,   500] loss: 96.843\n",
            "[117,  1000] loss: 97.759\n",
            "[117,  1500] loss: 97.246\n",
            "[117,  2000] loss: 97.704\n",
            "[117,  2500] loss: 97.833\n",
            "[117,  3000] loss: 97.604\n",
            "[118,   500] loss: 96.732\n",
            "[118,  1000] loss: 97.223\n",
            "[118,  1500] loss: 97.452\n",
            "[118,  2000] loss: 97.671\n",
            "[118,  2500] loss: 97.671\n",
            "[118,  3000] loss: 97.971\n",
            "[119,   500] loss: 96.931\n",
            "[119,  1000] loss: 97.151\n",
            "[119,  1500] loss: 97.400\n",
            "[119,  2000] loss: 97.747\n",
            "[119,  2500] loss: 97.875\n",
            "[119,  3000] loss: 97.474\n",
            "[120,   500] loss: 96.929\n",
            "[120,  1000] loss: 97.666\n",
            "[120,  1500] loss: 97.444\n",
            "[120,  2000] loss: 98.031\n",
            "[120,  2500] loss: 96.845\n",
            "[120,  3000] loss: 97.567\n",
            "[121,   500] loss: 97.378\n",
            "[121,  1000] loss: 97.299\n",
            "[121,  1500] loss: 97.359\n",
            "[121,  2000] loss: 97.394\n",
            "[121,  2500] loss: 97.178\n",
            "[121,  3000] loss: 97.795\n",
            "NLL: -95.3841645934581 active units: (10, 0)\n",
            "learning rate= 0.00019306977288832499\n",
            "[122,   500] loss: 96.360\n",
            "[122,  1000] loss: 96.259\n",
            "[122,  1500] loss: 97.269\n",
            "[122,  2000] loss: 96.885\n",
            "[122,  2500] loss: 97.236\n",
            "[122,  3000] loss: 97.088\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}